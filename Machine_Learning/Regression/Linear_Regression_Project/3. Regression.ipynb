{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db81bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fced26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8b19a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"housing.csv\")\n",
    "data['rooms_per_household'] = data['total_rooms']/data['households']\n",
    "data['population_per_household'] = data['population']/data['households']\n",
    "data['bedrooms_per_rooms'] = data['total_bedrooms']/data['total_rooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac91cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['latitude', 'longitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d84259",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(\"median_house_value\", axis=1)\n",
    "target = data['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77190693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'rooms_per_household',\n",
       " 'population_per_household',\n",
       " 'bedrooms_per_rooms']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.columns[:6]) + list(features.columns[7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb6cb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a34b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ocean_proximity'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "768a3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attr = np.array(list(features.columns[:6]) + list(features.columns[7:]))\n",
    "cat_attr = [features.columns[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3403add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline([\n",
    "    ('simple', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "pipe = ColumnTransformer([\n",
    "    ('num', num_pipe, num_attr),\n",
    "    ('cat', OneHotEncoder(), cat_attr)  # ['ocean']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81b36b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['housing_median_age', 'total_rooms', 'total_bedrooms',\n",
       "       'population', 'households', 'median_income', 'rooms_per_household',\n",
       "       'population_per_household', 'bedrooms_per_rooms'], dtype='<U24')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97b6ffb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98214266, -0.8048191 , -0.97247648, ...,  0.62855945,\n",
       "        -0.04959654, -1.14993031],\n",
       "       [-0.60701891,  2.0458901 ,  1.35714343, ...,  0.32704136,\n",
       "        -0.09251223, -0.99038135],\n",
       "       [ 1.85618152, -0.53574589, -0.82702426, ...,  1.15562047,\n",
       "        -0.02584253, -1.44586501],\n",
       "       ...,\n",
       "       [-0.92485123, -0.17499526, -0.12360781, ..., -0.09031802,\n",
       "        -0.0717345 ,  0.03870567],\n",
       "       [-0.84539315, -0.35559977, -0.30482697, ..., -0.04021111,\n",
       "        -0.09122515,  0.12050112],\n",
       "       [-1.00430931,  0.06840827,  0.18875678, ..., -0.07044252,\n",
       "        -0.04368215,  0.14290124]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pipe.fit_transform(features.loc[:, num_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe6ac732",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_trans = pipe.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6566da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 14)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fbb9771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 10)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6781f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48850.31800601671"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = ColumnTransformer([\n",
    "    ('num', num_pipe, ['housing_median_age', 'total_rooms', 'total_bedrooms',\n",
    "       'population', 'households', 'median_income', 'rooms_per_household',\n",
    "       'population_per_household', 'bedrooms_per_rooms']),\n",
    "    ('cat', OneHotEncoder(), ['ocean_proximity'])\n",
    "])\n",
    "final_feat = pipe.fit_transform(features)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(final_feat, target, random_state=100)\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "pred = model.predict(X_test)\n",
    "mean_absolute_error(Y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22077d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485002.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.max() - target.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a783eb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6590061766540446"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab2f2dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480, 14)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "938a7133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.84539315,  0.41815761, -0.1498369 ,  0.07110631, -0.13219192,\n",
       "         1.60773138,  0.99955196,  0.02729353, -1.37514572,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.34647803, -0.33863945, -0.3787453 , -0.47462052, -0.29959074,\n",
       "        -0.92619934, -0.20279065, -0.07357756, -0.23705756,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 1.85618152, -0.40648073, -0.27382895, -0.51965623, -0.2838971 ,\n",
       "        -0.8136071 , -0.38634205, -0.08954441,  0.49119329,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.2891866 , -0.45782008, -0.3405939 , -0.68567022, -0.31528438,\n",
       "         0.59703311, -0.44853992, -0.13077995,  0.48085408,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [ 0.10810379, -0.17178655, -0.22375524, -0.43841534, -0.30482196,\n",
       "         0.2176199 ,  0.19174325, -0.06211056, -0.29478884,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f083848b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cb12ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"linmodel.pkl\", 'wb') as fp:\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c66ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"pipe.pkl\", 'wb') as fp:\n",
    "    pickle.dump(pipe, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe5aa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\xa0\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x1asklearn.linear_model._base\\x94\\x8c\\x10LinearRegression\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\rfit_intercept\\x94\\x88\\x8c\\tnormalize\\x94\\x89\\x8c\\x06copy_X\\x94\\x88\\x8c\\x06n_jobs\\x94N\\x8c\\x08positive\\x94\\x89\\x8c\\x0en_features_in_\\x94K\\x0e\\x8c\\x05coef_\\x94\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x0c_reconstruct\\x94\\x93\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94K\\x00\\x85\\x94C\\x01b\\x94\\x87\\x94R\\x94(K\\x01K\\x0e\\x85\\x94h\\x0f\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89Cp_5\\x04\\x8b\\xaa;\\xcd@v\\xe0\\xe0\\xd9\\xd2\\xec\\xb0@\\xf5>-\\r\\x7f\\xcd\\x9e\\xc0Sj\\r\\x176.\\xe5\\xc0FM\\xdc\\xffq\\xd6\\xe6@DV\\x87#\\x7f\\x96\\xf3@\\x01x\\x16X\\x10\"\\xb3@\\x969\\xe2\\x91\\x01\\xf6\\x80\\xc0F\\\\M\\x8d\\x1d.\\xce@\\xcf\\xeb\\xcb%h\\xdb\\xd9\\xc0\\xd9LN\\x02O\\xea\\xf5\\xc0\\xdcT\\xcc}\\x99\\x16\\x02A\\xb6\\xa33~\\xc2l\\xd3\\xc0&\\xc8S\\x82\\xca\\x86\\xc7\\xc0\\x94t\\x94b\\x8c\\t_residues\\x94h\\x0eh\\x11K\\x00\\x85\\x94h\\x13\\x87\\x94R\\x94(K\\x01K\\x00\\x85\\x94h\\x1b\\x89C\\x00\\x94t\\x94b\\x8c\\x05rank_\\x94K\\r\\x8c\\tsingular_\\x94h\\x0eh\\x11K\\x00\\x85\\x94h\\x13\\x87\\x94R\\x94(K\\x01K\\x0e\\x85\\x94h\\x1b\\x89Cp2\\xefv\\x89\\x12\\xben@\\xf8\\x10>\\x01\\x99Ef@\\x8b\\xb5<PV\\xbd`@g\\x81\\x86\\x8b\\xd8\\x81]@\\xf7\\x85\\x1f\\xcd\\x8b\\xa0Z@\\xdant#\\x16\\xa4U@\\x9f2\\xe2\\xae\\xf6\\xccO@\\xb1Q\\xb6^\\x9a\\xa4H@1\\xf1~\\x86D\\x05G@THrXs\\x9aD@\\nQ\\xd4:g\\xc99@\\xd2\\xf4,\\xc8\\x04\\x050@\\xf5E|\\xc6/\\xf8\\xfe?\\xaaz\\xc1\\x0b<\\x96\\x0e=\\x94t\\x94b\\x8c\\nintercept_\\x94h\\x0c\\x8c\\x06scalar\\x94\\x93\\x94h\\x1bC\\x08\\xe6#\\xd7\\x9a\\xfe\\xa4\\x0eA\\x94\\x86\\x94R\\x94\\x8c\\x10_sklearn_version\\x94\\x8c\\x060.24.1\\x94ub.'\n"
     ]
    }
   ],
   "source": [
    "with open(\"linmodel.pkl\", 'rb') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0019f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n"
     ]
    }
   ],
   "source": [
    "with open(\"linmodel.pkl\", 'rb') as fp:\n",
    "    print(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c868cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "109ae8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logger', 'MemorizedResult', 'Memory', 'Parallel', 'PrintTime', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_deprecated_my_exceptions', '_memmapping_reducer', '_multiprocessing_helpers', '_parallel_backends', '_store_backends', 'backports', 'compressor', 'cpu_count', 'delayed', 'disk', 'dump', 'effective_n_jobs', 'executor', 'externals', 'func_inspect', 'hash', 'hashing', 'load', 'logger', 'memory', 'my_exceptions', 'numpy_pickle', 'numpy_pickle_compat', 'numpy_pickle_utils', 'os', 'parallel', 'parallel_backend', 'pool', 'register_compressor', 'register_parallel_backend', 'register_store_backend', 'wrap_non_picklable_objects']\n"
     ]
    }
   ],
   "source": [
    "print(dir(joblib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40b4500e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package joblib:\n",
      "\n",
      "NAME\n",
      "    joblib\n",
      "\n",
      "DESCRIPTION\n",
      "    Joblib is a set of tools to provide **lightweight pipelining in\n",
      "    Python**. In particular:\n",
      "    \n",
      "    1. transparent disk-caching of functions and lazy re-evaluation\n",
      "       (memoize pattern)\n",
      "    \n",
      "    2. easy simple parallel computing\n",
      "    \n",
      "    Joblib is optimized to be **fast** and **robust** on large\n",
      "    data in particular and has specific optimizations for `numpy` arrays. It is\n",
      "    **BSD-licensed**.\n",
      "    \n",
      "    \n",
      "        ==================== ===============================================\n",
      "        **Documentation:**       https://joblib.readthedocs.io\n",
      "    \n",
      "        **Download:**            https://pypi.python.org/pypi/joblib#downloads\n",
      "    \n",
      "        **Source code:**         https://github.com/joblib/joblib\n",
      "    \n",
      "        **Report issues:**       https://github.com/joblib/joblib/issues\n",
      "        ==================== ===============================================\n",
      "    \n",
      "    \n",
      "    Vision\n",
      "    --------\n",
      "    \n",
      "    The vision is to provide tools to easily achieve better performance and\n",
      "    reproducibility when working with long running jobs.\n",
      "    \n",
      "     *  **Avoid computing the same thing twice**: code is often rerun again and\n",
      "        again, for instance when prototyping computational-heavy jobs (as in\n",
      "        scientific development), but hand-crafted solutions to alleviate this\n",
      "        issue are error-prone and often lead to unreproducible results.\n",
      "    \n",
      "     *  **Persist to disk transparently**: efficiently persisting\n",
      "        arbitrary objects containing large data is hard. Using\n",
      "        joblib's caching mechanism avoids hand-written persistence and\n",
      "        implicitly links the file on disk to the execution context of\n",
      "        the original Python object. As a result, joblib's persistence is\n",
      "        good for resuming an application status or computational job, eg\n",
      "        after a crash.\n",
      "    \n",
      "    Joblib addresses these problems while **leaving your code and your flow\n",
      "    control as unmodified as possible** (no framework, no new paradigms).\n",
      "    \n",
      "    Main features\n",
      "    ------------------\n",
      "    \n",
      "    1) **Transparent and fast disk-caching of output value:** a memoize or\n",
      "       make-like functionality for Python functions that works well for\n",
      "       arbitrary Python objects, including very large numpy arrays. Separate\n",
      "       persistence and flow-execution logic from domain logic or algorithmic\n",
      "       code by writing the operations as a set of steps with well-defined\n",
      "       inputs and  outputs: Python functions. Joblib can save their\n",
      "       computation to disk and rerun it only if necessary::\n",
      "    \n",
      "          >>> from joblib import Memory\n",
      "          >>> cachedir = 'your_cache_dir_goes_here'\n",
      "          >>> mem = Memory(cachedir)\n",
      "          >>> import numpy as np\n",
      "          >>> a = np.vander(np.arange(3)).astype(np.float)\n",
      "          >>> square = mem.cache(np.square)\n",
      "          >>> b = square(a)                                   # doctest: +ELLIPSIS\n",
      "          ________________________________________________________________________________\n",
      "          [Memory] Calling square...\n",
      "          square(array([[0., 0., 1.],\n",
      "                 [1., 1., 1.],\n",
      "                 [4., 2., 1.]]))\n",
      "          ___________________________________________________________square - 0...s, 0.0min\n",
      "    \n",
      "          >>> c = square(a)\n",
      "          >>> # The above call did not trigger an evaluation\n",
      "    \n",
      "    2) **Embarrassingly parallel helper:** to make it easy to write readable\n",
      "       parallel code and debug it quickly::\n",
      "    \n",
      "          >>> from joblib import Parallel, delayed\n",
      "          >>> from math import sqrt\n",
      "          >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n",
      "          [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "    \n",
      "    \n",
      "    3) **Fast compressed Persistence**: a replacement for pickle to work\n",
      "       efficiently on Python objects containing large data (\n",
      "       *joblib.dump* & *joblib.load* ).\n",
      "    \n",
      "    ..\n",
      "        >>> import shutil ; shutil.rmtree(cachedir)\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _dask\n",
      "    _deprecated_format_stack\n",
      "    _deprecated_my_exceptions\n",
      "    _memmapping_reducer\n",
      "    _multiprocessing_helpers\n",
      "    _parallel_backends\n",
      "    _store_backends\n",
      "    backports\n",
      "    compressor\n",
      "    disk\n",
      "    executor\n",
      "    externals (package)\n",
      "    format_stack\n",
      "    func_inspect\n",
      "    hashing\n",
      "    logger\n",
      "    memory\n",
      "    my_exceptions\n",
      "    numpy_pickle\n",
      "    numpy_pickle_compat\n",
      "    numpy_pickle_utils\n",
      "    parallel\n",
      "    pool\n",
      "    test (package)\n",
      "    testing\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        joblib.logger.Logger\n",
      "            joblib.memory.MemorizedResult\n",
      "            joblib.memory.Memory\n",
      "            joblib.parallel.Parallel\n",
      "        joblib.logger.PrintTime\n",
      "        joblib.parallel.parallel_backend\n",
      "    \n",
      "    class Logger(builtins.object)\n",
      "     |  Logger(depth=3)\n",
      "     |  \n",
      "     |  Base class for logging messages.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, depth=3)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MemorizedResult(joblib.logger.Logger)\n",
      "     |  MemorizedResult(location, func, args_id, backend='local', mmap_mode=None, verbose=0, timestamp=None, metadata=None)\n",
      "     |  \n",
      "     |  Object representing a cached value.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  location: str\n",
      "     |      The location of joblib cache. Depends on the store backend used.\n",
      "     |  \n",
      "     |  func: function or str\n",
      "     |      function whose output is cached. The string case is intended only for\n",
      "     |      instanciation based on the output of repr() on another instance.\n",
      "     |      (namely eval(repr(memorized_instance)) works).\n",
      "     |  \n",
      "     |  argument_hash: str\n",
      "     |      hash of the function arguments.\n",
      "     |  \n",
      "     |  backend: str\n",
      "     |      Type of store backend for reading/writing cache files.\n",
      "     |      Default is 'local'.\n",
      "     |  \n",
      "     |  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n",
      "     |      The memmapping mode used when loading from cache numpy arrays. See\n",
      "     |      numpy.load for the meaning of the different values.\n",
      "     |  \n",
      "     |  verbose: int\n",
      "     |      verbosity level (0 means no message).\n",
      "     |  \n",
      "     |  timestamp, metadata: string\n",
      "     |      for internal use only.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MemorizedResult\n",
      "     |      joblib.logger.Logger\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, location, func, args_id, backend='local', mmap_mode=None, verbose=0, timestamp=None, metadata=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      Clear value from cache\n",
      "     |  \n",
      "     |  get(self)\n",
      "     |      Read value from cache and return it.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  argument_hash\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Memory(joblib.logger.Logger)\n",
      "     |  Memory(location=None, backend='local', cachedir=None, mmap_mode=None, compress=False, verbose=1, bytes_limit=None, backend_options=None)\n",
      "     |  \n",
      "     |  A context object for caching a function's return value each time it\n",
      "     |  is called with the same input arguments.\n",
      "     |  \n",
      "     |  All values are cached on the filesystem, in a deep directory\n",
      "     |  structure.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <memory>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  location: str or None\n",
      "     |      The path of the base directory to use as a data store\n",
      "     |      or None. If None is given, no caching is done and\n",
      "     |      the Memory object is completely transparent. This option\n",
      "     |      replaces cachedir since version 0.12.\n",
      "     |  \n",
      "     |  backend: str, optional\n",
      "     |      Type of store backend for reading/writing cache files.\n",
      "     |      Default: 'local'.\n",
      "     |      The 'local' backend is using regular filesystem operations to\n",
      "     |      manipulate data (open, mv, etc) in the backend.\n",
      "     |  \n",
      "     |  cachedir: str or None, optional\n",
      "     |  \n",
      "     |      .. deprecated: 0.12\n",
      "     |          'cachedir' has been deprecated in 0.12 and will be\n",
      "     |          removed in 0.14. Use the 'location' parameter instead.\n",
      "     |  \n",
      "     |  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n",
      "     |      The memmapping mode used when loading from cache\n",
      "     |      numpy arrays. See numpy.load for the meaning of the\n",
      "     |      arguments.\n",
      "     |  \n",
      "     |  compress: boolean, or integer, optional\n",
      "     |      Whether to zip the stored data on disk. If an integer is\n",
      "     |      given, it should be between 1 and 9, and sets the amount\n",
      "     |      of compression. Note that compressed arrays cannot be\n",
      "     |      read by memmapping.\n",
      "     |  \n",
      "     |  verbose: int, optional\n",
      "     |      Verbosity flag, controls the debug messages that are issued\n",
      "     |      as functions are evaluated.\n",
      "     |  \n",
      "     |  bytes_limit: int, optional\n",
      "     |      Limit in bytes of the size of the cache.\n",
      "     |  \n",
      "     |  backend_options: dict, optional\n",
      "     |      Contains a dictionnary of named parameters used to configure\n",
      "     |      the store backend.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Memory\n",
      "     |      joblib.logger.Logger\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      We don't store the timestamp when pickling, to avoid the hash\n",
      "     |      depending from it.\n",
      "     |  \n",
      "     |  __init__(self, location=None, backend='local', cachedir=None, mmap_mode=None, compress=False, verbose=1, bytes_limit=None, backend_options=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cache(self, func=None, ignore=None, verbose=None, mmap_mode=False)\n",
      "     |      Decorates the given function func to only compute its return\n",
      "     |      value for input arguments not cached on disk.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func: callable, optional\n",
      "     |          The function to be decorated\n",
      "     |      ignore: list of strings\n",
      "     |          A list of arguments name to ignore in the hashing\n",
      "     |      verbose: integer, optional\n",
      "     |          The verbosity mode of the function. By default that\n",
      "     |          of the memory object is used.\n",
      "     |      mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n",
      "     |          The memmapping mode used when loading from cache\n",
      "     |          numpy arrays. See numpy.load for the meaning of the\n",
      "     |          arguments. By default that of the memory object is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      decorated_func: MemorizedFunc object\n",
      "     |          The returned object is a MemorizedFunc object, that is\n",
      "     |          callable (behaves like a function), but offers extra\n",
      "     |          methods for cache lookup and management. See the\n",
      "     |          documentation for :class:`joblib.memory.MemorizedFunc`.\n",
      "     |  \n",
      "     |  clear(self, warn=True)\n",
      "     |      Erase the complete cache directory.\n",
      "     |  \n",
      "     |  eval(self, func, *args, **kwargs)\n",
      "     |      Eval function func with arguments `*args` and `**kwargs`,\n",
      "     |      in the context of the memory.\n",
      "     |      \n",
      "     |      This method works similarly to the builtin `apply`, except\n",
      "     |      that the function is called only if the cache is not\n",
      "     |      up to date.\n",
      "     |  \n",
      "     |  reduce_size(self)\n",
      "     |      Remove cache elements to make cache size fit in ``bytes_limit``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  cachedir\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Parallel(joblib.logger.Logger)\n",
      "     |  Parallel(n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch='2 * n_jobs', batch_size='auto', temp_folder=None, max_nbytes='1M', mmap_mode='r', prefer=None, require=None)\n",
      "     |  \n",
      "     |  Helper class for readable parallel mapping.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <parallel>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  -----------\n",
      "     |  n_jobs: int, default: None\n",
      "     |      The maximum number of concurrently running jobs, such as the number\n",
      "     |      of Python worker processes when backend=\"multiprocessing\"\n",
      "     |      or the size of the thread-pool when backend=\"threading\".\n",
      "     |      If -1 all CPUs are used. If 1 is given, no parallel computing code\n",
      "     |      is used at all, which is useful for debugging. For n_jobs below -1,\n",
      "     |      (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n",
      "     |      CPUs but one are used.\n",
      "     |      None is a marker for 'unset' that will be interpreted as n_jobs=1\n",
      "     |      (sequential execution) unless the call is performed under a\n",
      "     |      parallel_backend context manager that sets another value for\n",
      "     |      n_jobs.\n",
      "     |  backend: str, ParallelBackendBase instance or None, default: 'loky'\n",
      "     |      Specify the parallelization backend implementation.\n",
      "     |      Supported backends are:\n",
      "     |  \n",
      "     |      - \"loky\" used by default, can induce some\n",
      "     |        communication and memory overhead when exchanging input and\n",
      "     |        output data with the worker Python processes.\n",
      "     |      - \"multiprocessing\" previous process-based backend based on\n",
      "     |        `multiprocessing.Pool`. Less robust than `loky`.\n",
      "     |      - \"threading\" is a very low-overhead backend but it suffers\n",
      "     |        from the Python Global Interpreter Lock if the called function\n",
      "     |        relies a lot on Python objects. \"threading\" is mostly useful\n",
      "     |        when the execution bottleneck is a compiled extension that\n",
      "     |        explicitly releases the GIL (for instance a Cython loop wrapped\n",
      "     |        in a \"with nogil\" block or an expensive call to a library such\n",
      "     |        as NumPy).\n",
      "     |      - finally, you can register backends by calling\n",
      "     |        register_parallel_backend. This will allow you to implement\n",
      "     |        a backend of your liking.\n",
      "     |  \n",
      "     |      It is not recommended to hard-code the backend name in a call to\n",
      "     |      Parallel in a library. Instead it is recommended to set soft hints\n",
      "     |      (prefer) or hard constraints (require) so as to make it possible\n",
      "     |      for library users to change the backend from the outside using the\n",
      "     |      parallel_backend context manager.\n",
      "     |  prefer: str in {'processes', 'threads'} or None, default: None\n",
      "     |      Soft hint to choose the default backend if no specific backend\n",
      "     |      was selected with the parallel_backend context manager. The\n",
      "     |      default process-based backend is 'loky' and the default\n",
      "     |      thread-based backend is 'threading'. Ignored if the ``backend``\n",
      "     |      parameter is specified.\n",
      "     |  require: 'sharedmem' or None, default None\n",
      "     |      Hard constraint to select the backend. If set to 'sharedmem',\n",
      "     |      the selected backend will be single-host and thread-based even\n",
      "     |      if the user asked for a non-thread based backend with\n",
      "     |      parallel_backend.\n",
      "     |  verbose: int, optional\n",
      "     |      The verbosity level: if non zero, progress messages are\n",
      "     |      printed. Above 50, the output is sent to stdout.\n",
      "     |      The frequency of the messages increases with the verbosity level.\n",
      "     |      If it more than 10, all iterations are reported.\n",
      "     |  timeout: float, optional\n",
      "     |      Timeout limit for each task to complete.  If any task takes longer\n",
      "     |      a TimeOutError will be raised. Only applied when n_jobs != 1\n",
      "     |  pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n",
      "     |      The number of batches (of tasks) to be pre-dispatched.\n",
      "     |      Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n",
      "     |      default and the workers should never starve.\n",
      "     |  batch_size: int or 'auto', default: 'auto'\n",
      "     |      The number of atomic tasks to dispatch at once to each\n",
      "     |      worker. When individual evaluations are very fast, dispatching\n",
      "     |      calls to workers can be slower than sequential computation because\n",
      "     |      of the overhead. Batching fast computations together can mitigate\n",
      "     |      this.\n",
      "     |      The ``'auto'`` strategy keeps track of the time it takes for a batch\n",
      "     |      to complete, and dynamically adjusts the batch size to keep the time\n",
      "     |      on the order of half a second, using a heuristic. The initial batch\n",
      "     |      size is 1.\n",
      "     |      ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n",
      "     |      batches of a single task at a time as the threading backend has\n",
      "     |      very little overhead and using larger batch size has not proved to\n",
      "     |      bring any gain in that case.\n",
      "     |  temp_folder: str, optional\n",
      "     |      Folder to be used by the pool for memmapping large arrays\n",
      "     |      for sharing memory with worker processes. If None, this will try in\n",
      "     |      order:\n",
      "     |  \n",
      "     |      - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n",
      "     |        variable,\n",
      "     |      - /dev/shm if the folder exists and is writable: this is a\n",
      "     |        RAM disk filesystem available by default on modern Linux\n",
      "     |        distributions,\n",
      "     |      - the default system temporary folder that can be\n",
      "     |        overridden with TMP, TMPDIR or TEMP environment\n",
      "     |        variables, typically /tmp under Unix operating systems.\n",
      "     |  \n",
      "     |      Only active when backend=\"loky\" or \"multiprocessing\".\n",
      "     |  max_nbytes int, str, or None, optional, 1M by default\n",
      "     |      Threshold on the size of arrays passed to the workers that\n",
      "     |      triggers automated memory mapping in temp_folder. Can be an int\n",
      "     |      in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n",
      "     |      Use None to disable memmapping of large arrays.\n",
      "     |      Only active when backend=\"loky\" or \"multiprocessing\".\n",
      "     |  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n",
      "     |      Memmapping mode for numpy arrays passed to workers.\n",
      "     |      See 'max_nbytes' parameter documentation for more details.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  This object uses workers to compute in parallel the application of a\n",
      "     |  function to many different arguments. The main functionality it brings\n",
      "     |  in addition to using the raw multiprocessing or concurrent.futures API\n",
      "     |  are (see examples for details):\n",
      "     |  \n",
      "     |  * More readable code, in particular since it avoids\n",
      "     |    constructing list of arguments.\n",
      "     |  \n",
      "     |  * Easier debugging:\n",
      "     |      - informative tracebacks even when the error happens on\n",
      "     |        the client side\n",
      "     |      - using 'n_jobs=1' enables to turn off parallel computing\n",
      "     |        for debugging without changing the codepath\n",
      "     |      - early capture of pickling errors\n",
      "     |  \n",
      "     |  * An optional progress meter.\n",
      "     |  \n",
      "     |  * Interruption of multiprocesses jobs with 'Ctrl-C'\n",
      "     |  \n",
      "     |  * Flexible pickling control for the communication to and from\n",
      "     |    the worker processes.\n",
      "     |  \n",
      "     |  * Ability to use shared memory efficiently with worker\n",
      "     |    processes for large numpy-based datastructures.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  A simple example:\n",
      "     |  \n",
      "     |  >>> from math import sqrt\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n",
      "     |  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "     |  \n",
      "     |  Reshaping the output when the function has several return\n",
      "     |  values:\n",
      "     |  \n",
      "     |  >>> from math import modf\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n",
      "     |  >>> res, i = zip(*r)\n",
      "     |  >>> res\n",
      "     |  (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n",
      "     |  >>> i\n",
      "     |  (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n",
      "     |  \n",
      "     |  The progress meter: the higher the value of `verbose`, the more\n",
      "     |  messages:\n",
      "     |  \n",
      "     |  >>> from time import sleep\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\n",
      "     |  [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n",
      "     |  [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\n",
      "     |  [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\n",
      "     |  \n",
      "     |  Traceback example, note how the line of the error is indicated\n",
      "     |  as well as the values of the parameter passed to the function that\n",
      "     |  triggered the exception, even though the traceback happens in the\n",
      "     |  child process:\n",
      "     |  \n",
      "     |  >>> from heapq import nlargest\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n",
      "     |  #...\n",
      "     |  ---------------------------------------------------------------------------\n",
      "     |  Sub-process traceback:\n",
      "     |  ---------------------------------------------------------------------------\n",
      "     |  TypeError                                          Mon Nov 12 11:37:46 2012\n",
      "     |  PID: 12934                                    Python 2.7.3: /usr/bin/python\n",
      "     |  ...........................................................................\n",
      "     |  /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n",
      "     |      419         if n >= size:\n",
      "     |      420             return sorted(iterable, key=key, reverse=True)[:n]\n",
      "     |      421\n",
      "     |      422     # When key is none, use simpler decoration\n",
      "     |      423     if key is None:\n",
      "     |  --> 424         it = izip(iterable, count(0,-1))                    # decorate\n",
      "     |      425         result = _nlargest(n, it)\n",
      "     |      426         return map(itemgetter(0), result)                   # undecorate\n",
      "     |      427\n",
      "     |      428     # General case, slowest method\n",
      "     |   TypeError: izip argument #1 must support iteration\n",
      "     |  ___________________________________________________________________________\n",
      "     |  \n",
      "     |  \n",
      "     |  Using pre_dispatch in a producer/consumer situation, where the\n",
      "     |  data is generated on the fly. Note how the producer is first\n",
      "     |  called 3 times before the parallel loop is initiated, and then\n",
      "     |  called to generate new data on the fly:\n",
      "     |  \n",
      "     |  >>> from math import sqrt\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> def producer():\n",
      "     |  ...     for i in range(6):\n",
      "     |  ...         print('Produced %s' % i)\n",
      "     |  ...         yield i\n",
      "     |  >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n",
      "     |  ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n",
      "     |  Produced 0\n",
      "     |  Produced 1\n",
      "     |  Produced 2\n",
      "     |  [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n",
      "     |  Produced 3\n",
      "     |  [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n",
      "     |  Produced 4\n",
      "     |  [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n",
      "     |  Produced 5\n",
      "     |  [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n",
      "     |  [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\n",
      "     |  [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Parallel\n",
      "     |      joblib.logger.Logger\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, iterable)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, exc_type, exc_value, traceback)\n",
      "     |  \n",
      "     |  __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch='2 * n_jobs', batch_size='auto', temp_folder=None, max_nbytes='1M', mmap_mode='r', prefer=None, require=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  dispatch_next(self)\n",
      "     |      Dispatch more data for parallel processing\n",
      "     |      \n",
      "     |      This method is meant to be called concurrently by the multiprocessing\n",
      "     |      callback. We rely on the thread-safety of dispatch_one_batch to protect\n",
      "     |      against concurrent consumption of the unprotected iterator.\n",
      "     |  \n",
      "     |  dispatch_one_batch(self, iterator)\n",
      "     |      Prefetch the tasks for the next batch and dispatch them.\n",
      "     |      \n",
      "     |      The effective size of the batch is computed here.\n",
      "     |      If there are no more jobs to dispatch, return False, else return True.\n",
      "     |      \n",
      "     |      The iterator consumption and dispatching is protected by the same\n",
      "     |      lock so calling this function should be thread safe.\n",
      "     |  \n",
      "     |  print_progress(self)\n",
      "     |      Display the process of the parallel execution only a fraction\n",
      "     |      of time, controlled by self.verbose.\n",
      "     |  \n",
      "     |  retrieve(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PrintTime(builtins.object)\n",
      "     |  PrintTime(logfile=None, logdir=None)\n",
      "     |  \n",
      "     |  Print and log messages while keeping track of time.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, msg='', total=False)\n",
      "     |      Print the time elapsed between the last call and the current\n",
      "     |      call, with an optional message.\n",
      "     |  \n",
      "     |  __init__(self, logfile=None, logdir=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class parallel_backend(builtins.object)\n",
      "     |  parallel_backend(backend, n_jobs=-1, inner_max_num_threads=None, **backend_params)\n",
      "     |  \n",
      "     |  Change the default backend used by Parallel inside a with block.\n",
      "     |  \n",
      "     |  If ``backend`` is a string it must match a previously registered\n",
      "     |  implementation using the ``register_parallel_backend`` function.\n",
      "     |  \n",
      "     |  By default the following backends are available:\n",
      "     |  \n",
      "     |  - 'loky': single-host, process-based parallelism (used by default),\n",
      "     |  - 'threading': single-host, thread-based parallelism,\n",
      "     |  - 'multiprocessing': legacy single-host, process-based parallelism.\n",
      "     |  \n",
      "     |  'loky' is recommended to run functions that manipulate Python objects.\n",
      "     |  'threading' is a low-overhead alternative that is most efficient for\n",
      "     |  functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n",
      "     |  CPU-bound code in a few calls to native code that explicitly releases the\n",
      "     |  GIL.\n",
      "     |  \n",
      "     |  In addition, if the `dask` and `distributed` Python packages are installed,\n",
      "     |  it is possible to use the 'dask' backend for better scheduling of nested\n",
      "     |  parallel calls without over-subscription and potentially distribute\n",
      "     |  parallel calls over a networked cluster of several hosts.\n",
      "     |  \n",
      "     |  It is also possible to use the distributed 'ray' backend for distributing\n",
      "     |  the workload to a cluster of nodes. To use the 'ray' joblib backend add\n",
      "     |  the following lines::\n",
      "     |  \n",
      "     |   >>> from ray.util.joblib import register_ray  # doctest: +SKIP\n",
      "     |   >>> register_ray()  # doctest: +SKIP\n",
      "     |   >>> with parallel_backend(\"ray\"):  # doctest: +SKIP\n",
      "     |   ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n",
      "     |   [-1, -2, -3, -4, -5]\n",
      "     |  \n",
      "     |  Alternatively the backend can be passed directly as an instance.\n",
      "     |  \n",
      "     |  By default all available workers will be used (``n_jobs=-1``) unless the\n",
      "     |  caller passes an explicit value for the ``n_jobs`` parameter.\n",
      "     |  \n",
      "     |  This is an alternative to passing a ``backend='backend_name'`` argument to\n",
      "     |  the ``Parallel`` class constructor. It is particularly useful when calling\n",
      "     |  into library code that uses joblib internally but does not expose the\n",
      "     |  backend argument in its own API.\n",
      "     |  \n",
      "     |  >>> from operator import neg\n",
      "     |  >>> with parallel_backend('threading'):\n",
      "     |  ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n",
      "     |  ...\n",
      "     |  [-1, -2, -3, -4, -5]\n",
      "     |  \n",
      "     |  Warning: this function is experimental and subject to change in a future\n",
      "     |  version of joblib.\n",
      "     |  \n",
      "     |  Joblib also tries to limit the oversubscription by limiting the number of\n",
      "     |  threads usable in some third-party library threadpools like OpenBLAS, MKL\n",
      "     |  or OpenMP. The default limit in each worker is set to\n",
      "     |  ``max(cpu_count() // effective_n_jobs, 1)`` but this limit can be\n",
      "     |  overwritten with the ``inner_max_num_threads`` argument which will be used\n",
      "     |  to set this limit in the child processes.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.10\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, type, value, traceback)\n",
      "     |  \n",
      "     |  __init__(self, backend, n_jobs=-1, inner_max_num_threads=None, **backend_params)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  unregister(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    cpu_count(only_physical_cores=False)\n",
      "        Return the number of CPUs.\n",
      "        \n",
      "        This delegates to loky.cpu_count that takes into account additional\n",
      "        constraints such as Linux CFS scheduler quotas (typically set by container\n",
      "        runtimes such as docker) and CPU affinity (for instance using the taskset\n",
      "        command on Linux).\n",
      "        \n",
      "        If only_physical_cores is True, do not take hyperthreading / SMT logical\n",
      "        cores into account.\n",
      "    \n",
      "    delayed(function)\n",
      "        Decorator used to capture the arguments of a function.\n",
      "    \n",
      "    dump(value, filename, compress=0, protocol=None, cache_size=None)\n",
      "        Persist an arbitrary Python object into one file.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <persistence>`.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        value: any Python object\n",
      "            The object to store to disk.\n",
      "        filename: str, pathlib.Path, or file object.\n",
      "            The file object or path of the file in which it is to be stored.\n",
      "            The compression method corresponding to one of the supported filename\n",
      "            extensions ('.z', '.gz', '.bz2', '.xz' or '.lzma') will be used\n",
      "            automatically.\n",
      "        compress: int from 0 to 9 or bool or 2-tuple, optional\n",
      "            Optional compression level for the data. 0 or False is no compression.\n",
      "            Higher value means more compression, but also slower read and\n",
      "            write times. Using a value of 3 is often a good compromise.\n",
      "            See the notes for more details.\n",
      "            If compress is True, the compression level used is 3.\n",
      "            If compress is a 2-tuple, the first element must correspond to a string\n",
      "            between supported compressors (e.g 'zlib', 'gzip', 'bz2', 'lzma'\n",
      "            'xz'), the second element must be an integer from 0 to 9, corresponding\n",
      "            to the compression level.\n",
      "        protocol: int, optional\n",
      "            Pickle protocol, see pickle.dump documentation for more details.\n",
      "        cache_size: positive int, optional\n",
      "            This option is deprecated in 0.10 and has no effect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        filenames: list of strings\n",
      "            The list of file names in which the data is stored. If\n",
      "            compress is false, each array is stored in a different file.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        joblib.load : corresponding loader\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Memmapping on load cannot be used for compressed files. Thus\n",
      "        using compression can significantly slow down loading. In\n",
      "        addition, compressed files take extra extra memory during\n",
      "        dump and load.\n",
      "    \n",
      "    effective_n_jobs(n_jobs=-1)\n",
      "        Determine the number of jobs that can actually run in parallel\n",
      "        \n",
      "        n_jobs is the number of workers requested by the callers. Passing n_jobs=-1\n",
      "        means requesting all available workers for instance matching the number of\n",
      "        CPU cores on the worker host(s).\n",
      "        \n",
      "        This method should return a guesstimate of the number of workers that can\n",
      "        actually perform work concurrently with the currently enabled default\n",
      "        backend. The primary use case is to make it possible for the caller to know\n",
      "        in how many chunks to slice the work.\n",
      "        \n",
      "        In general working on larger data chunks is more efficient (less scheduling\n",
      "        overhead and better use of CPU cache prefetching heuristics) as long as all\n",
      "        the workers have enough work to do.\n",
      "        \n",
      "        Warning: this function is experimental and subject to change in a future\n",
      "        version of joblib.\n",
      "        \n",
      "        .. versionadded:: 0.10\n",
      "    \n",
      "    hash(obj, hash_name='md5', coerce_mmap=False)\n",
      "        Quick calculation of a hash to identify uniquely Python objects\n",
      "        containing numpy arrays.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        hash_name: 'md5' or 'sha1'\n",
      "            Hashing algorithm used. sha1 is supposedly safer, but md5 is\n",
      "            faster.\n",
      "        coerce_mmap: boolean\n",
      "            Make no difference between np.memmap and np.ndarray\n",
      "    \n",
      "    load(filename, mmap_mode=None)\n",
      "        Reconstruct a Python object from a file persisted with joblib.dump.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <persistence>`.\n",
      "        \n",
      "        WARNING: joblib.load relies on the pickle module and can therefore\n",
      "        execute arbitrary Python code. It should therefore never be used\n",
      "        to load files from untrusted sources.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        filename: str, pathlib.Path, or file object.\n",
      "            The file object or path of the file from which to load the object\n",
      "        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n",
      "            If not None, the arrays are memory-mapped from the disk. This\n",
      "            mode has no effect for compressed files. Note that in this\n",
      "            case the reconstructed object might no longer match exactly\n",
      "            the originally pickled object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result: any Python object\n",
      "            The object stored in the file.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        joblib.dump : function to save an object\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        This function can load numpy array files saved separately during the\n",
      "        dump. If the mmap_mode argument is given, it is passed to np.load and\n",
      "        arrays are loaded as memmaps. As a consequence, the reconstructed\n",
      "        object might not match the original pickled object. Note that if the\n",
      "        file was saved with compression, the arrays cannot be memmapped.\n",
      "    \n",
      "    register_compressor(compressor_name, compressor, force=False)\n",
      "        Register a new compressor.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        compressor_name: str.\n",
      "            The name of the compressor.\n",
      "        compressor: CompressorWrapper\n",
      "            An instance of a 'CompressorWrapper'.\n",
      "    \n",
      "    register_parallel_backend(name, factory, make_default=False)\n",
      "        Register a new Parallel backend factory.\n",
      "        \n",
      "        The new backend can then be selected by passing its name as the backend\n",
      "        argument to the Parallel class. Moreover, the default backend can be\n",
      "        overwritten globally by setting make_default=True.\n",
      "        \n",
      "        The factory can be any callable that takes no argument and return an\n",
      "        instance of ``ParallelBackendBase``.\n",
      "        \n",
      "        Warning: this function is experimental and subject to change in a future\n",
      "        version of joblib.\n",
      "        \n",
      "        .. versionadded:: 0.10\n",
      "    \n",
      "    register_store_backend(backend_name, backend)\n",
      "        Extend available store backends.\n",
      "        \n",
      "        The Memory, MemorizeResult and MemorizeFunc objects are designed to be\n",
      "        agnostic to the type of store used behind. By default, the local file\n",
      "        system is used but this function gives the possibility to extend joblib's\n",
      "        memory pattern with other types of storage such as cloud storage (S3, GCS,\n",
      "        OpenStack, HadoopFS, etc) or blob DBs.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        backend_name: str\n",
      "            The name identifying the store backend being registered. For example,\n",
      "            'local' is used with FileSystemStoreBackend.\n",
      "        backend: StoreBackendBase subclass\n",
      "            The name of a class that implements the StoreBackendBase interface.\n",
      "    \n",
      "    wrap_non_picklable_objects(obj, keep_wrapper=True)\n",
      "        Wrapper for non-picklable object to use cloudpickle to serialize them.\n",
      "        \n",
      "        Note that this wrapper tends to slow down the serialization process as it\n",
      "        is done with cloudpickle which is typically slower compared to pickle. The\n",
      "        proper way to solve serialization issues is to avoid defining functions and\n",
      "        objects in the main scripts and to implement __reduce__ functions for\n",
      "        complex classes.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Memory', 'MemorizedResult', 'PrintTime', 'Logger', 'hash',...\n",
      "\n",
      "VERSION\n",
      "    1.0.1\n",
      "\n",
      "FILE\n",
      "    c:\\anaconda\\lib\\site-packages\\joblib\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "138113ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"linmodel.pkl\", 'wb') as fp:\n",
    "    joblib.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2bdf7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95<\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x1asklearn.linear_model._base\\x94\\x8c\\x10LinearRegression\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\rfit_intercept\\x94\\x88\\x8c\\tnormalize\\x94\\x89\\x8c\\x06copy_X\\x94\\x88\\x8c\\x06n_jobs\\x94N\\x8c\\x08positive\\x94\\x89\\x8c\\x0en_features_in_\\x94K\\x0e\\x8c\\x05coef_\\x94\\x8c\\x13joblib.numpy_pickle\\x94\\x8c\\x11NumpyArrayWrapper\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x08subclass\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94\\x8c\\x05shape\\x94K\\x0e\\x85\\x94\\x8c\\x05order\\x94\\x8c\\x01C\\x94\\x8c\\x05dtype\\x94h\\x12h\\x19\\x93\\x94\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x8c\\nallow_mmap\\x94\\x88ub_5\\x04\\x8b\\xaa;\\xcd@v\\xe0\\xe0\\xd9\\xd2\\xec\\xb0@\\xf5>-\\r\\x7f\\xcd\\x9e\\xc0Sj\\r\\x176.\\xe5\\xc0FM\\xdc\\xffq\\xd6\\xe6@DV\\x87#\\x7f\\x96\\xf3@\\x01x\\x16X\\x10\"\\xb3@\\x969\\xe2\\x91\\x01\\xf6\\x80\\xc0F\\\\M\\x8d\\x1d.\\xce@\\xcf\\xeb\\xcb%h\\xdb\\xd9\\xc0\\xd9LN\\x02O\\xea\\xf5\\xc0\\xdcT\\xcc}\\x99\\x16\\x02A\\xb6\\xa33~\\xc2l\\xd3\\xc0&\\xc8S\\x82\\xca\\x86\\xc7\\xc0\\x95+\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\t_residues\\x94h\\x0e)\\x81\\x94}\\x94(h\\x11h\\x14h\\x15K\\x00\\x85\\x94h\\x17h\\x18h\\x19h\\x1dh \\x88ub\\x955\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x05rank_\\x94K\\r\\x8c\\tsingular_\\x94h\\x0e)\\x81\\x94}\\x94(h\\x11h\\x14h\\x15K\\x0e\\x85\\x94h\\x17h\\x18h\\x19h\\x1dh \\x88ub2\\xefv\\x89\\x12\\xben@\\xf8\\x10>\\x01\\x99Ef@\\x8b\\xb5<PV\\xbd`@g\\x81\\x86\\x8b\\xd8\\x81]@\\xf7\\x85\\x1f\\xcd\\x8b\\xa0Z@\\xdant#\\x16\\xa4U@\\x9f2\\xe2\\xae\\xf6\\xccO@\\xb1Q\\xb6^\\x9a\\xa4H@1\\xf1~\\x86D\\x05G@THrXs\\x9aD@\\nQ\\xd4:g\\xc99@\\xd2\\xf4,\\xc8\\x04\\x050@\\xf5E|\\xc6/\\xf8\\xfe?\\xaaz\\xc1\\x0b<\\x96\\x0e=\\x95`\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\nintercept_\\x94\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x06scalar\\x94\\x93\\x94h\\x1dC\\x08\\xe6#\\xd7\\x9a\\xfe\\xa4\\x0eA\\x94\\x86\\x94R\\x94\\x8c\\x10_sklearn_version\\x94\\x8c\\x060.24.1\\x94ub.'\n"
     ]
    }
   ],
   "source": [
    "with open(\"linmodel.pkl\", 'rb') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1ba8bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n"
     ]
    }
   ],
   "source": [
    "with open(\"linmodel.pkl\", 'rb') as fp:\n",
    "    print(joblib.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "728f9b21",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a3eacd5d2272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pipe.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"pipe.pkl\", 'wb') as fp:\n",
    "    joblib.dump(pipe, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4d5146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48850.20770609444"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = ColumnTransformer([\n",
    "    ('num', num_pipe, ['housing_median_age', 'total_rooms', 'total_bedrooms',\n",
    "       'population', 'households', 'median_income', 'rooms_per_household',\n",
    "       'population_per_household', 'bedrooms_per_rooms']),\n",
    "    ('cat', OneHotEncoder(), ['ocean_proximity'])\n",
    "])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target, random_state=100)\n",
    "\n",
    "X_train = pipe.fit_transform(X_train)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "X_test = pipe.transform(X_test)\n",
    "pred = model.predict(X_test)\n",
    "mean_absolute_error(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e0799c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing_median_age              41.0\n",
       "total_rooms                    880.0\n",
       "total_bedrooms                 129.0\n",
       "population                     322.0\n",
       "households                     126.0\n",
       "median_income                 8.3252\n",
       "median_house_value          452600.0\n",
       "ocean_proximity             NEAR BAY\n",
       "rooms_per_household         6.984127\n",
       "population_per_household    2.555556\n",
       "bedrooms_per_rooms          0.146591\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07de82bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ocean_proximity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8d6201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>population_per_household</th>\n",
       "      <th>bedrooms_per_rooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>0.146591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>0.155797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>0.129516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>0.184458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>0.172096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   housing_median_age  total_rooms  total_bedrooms  population  households  \\\n",
       "0                41.0        880.0           129.0       322.0       126.0   \n",
       "1                21.0       7099.0          1106.0      2401.0      1138.0   \n",
       "2                52.0       1467.0           190.0       496.0       177.0   \n",
       "3                52.0       1274.0           235.0       558.0       219.0   \n",
       "4                52.0       1627.0           280.0       565.0       259.0   \n",
       "\n",
       "   median_income ocean_proximity  rooms_per_household  \\\n",
       "0         8.3252        NEAR BAY             6.984127   \n",
       "1         8.3014        NEAR BAY             6.238137   \n",
       "2         7.2574        NEAR BAY             8.288136   \n",
       "3         5.6431        NEAR BAY             5.817352   \n",
       "4         3.8462        NEAR BAY             6.281853   \n",
       "\n",
       "   population_per_household  bedrooms_per_rooms  \n",
       "0                  2.555556            0.146591  \n",
       "1                  2.109842            0.155797  \n",
       "2                  2.802260            0.129516  \n",
       "3                  2.547945            0.184458  \n",
       "4                  2.181467            0.172096  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dffe3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [[42, 130, 130, 400, 130, 9, 'NEAR BAY', 1.0, 3.076923076923077, 1.0]]\n",
    "df = pd.DataFrame(values, columns=features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24db2ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([656673.41618051])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(pipe.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7b8d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        452600.0\n",
       "1        358500.0\n",
       "2        352100.0\n",
       "3        341300.0\n",
       "4        342200.0\n",
       "           ...   \n",
       "20635     78100.0\n",
       "20636     77100.0\n",
       "20637     92300.0\n",
       "20638     84700.0\n",
       "20639     89400.0\n",
       "Name: median_house_value, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acf914a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>population_per_household</th>\n",
       "      <th>bedrooms_per_rooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>400</td>\n",
       "      <td>130</td>\n",
       "      <td>9</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   housing_median_age  total_rooms  total_bedrooms  population  households  \\\n",
       "0                  42          130             130         400         130   \n",
       "\n",
       "   median_income ocean_proximity  rooms_per_household  \\\n",
       "0              9        NEAR BAY                  1.0   \n",
       "\n",
       "   population_per_household  bedrooms_per_rooms  \n",
       "0                  3.076923                 1.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65bff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
